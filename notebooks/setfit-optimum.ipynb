{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76571396-8f54-40ed-9e81-6c7531e6eaee",
   "metadata": {
    "id": "76571396-8f54-40ed-9e81-6c7531e6eaee"
   },
   "source": [
    "# Accelerate SetFit Models with ðŸ¤— Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f283-4736-4d9d-854e-5582280b074d",
   "metadata": {},
   "source": [
    "In this notebook, we'll show how you can use ðŸ¤— [Optimum Intel](https://github.com/huggingface/optimum-intel) to speed up your SetFit models on Intel CPUs.\\\n",
    "Optimum Intel is the interface between the ðŸ¤— Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n",
    "\n",
    "In our experiments, we will be using a fine-tuned SetFit model based on `bge-small-en-v1.5` sentence transformer model and observe our performance increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874cea-d8d1-4fdf-b374-7772811fe7b7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbde933-a2f1-4159-9b93-cfcf73f28c79",
   "metadata": {},
   "source": [
    "* Linux OS\n",
    "* Python 3.9+\n",
    "* An Intel Xeon Processor which supports [bfloat16](https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-on-xeon-processors-with-bfloat16.html)\n",
    "\n",
    "Since Google Colab doesn't meet the hardware requirement, it's incompatible with this notebook and will not see speedups from our optimizations.\n",
    "\n",
    "Please run the following to ensure you meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d9247a-b053-46af-89f6-e57067ecb8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook requirements  met.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "os_info = !uname -o\n",
    "os_type = os_info[0]\n",
    "is_os_ok = \"linux\" in os_type.lower()\n",
    "is_python_ver_ok = sys.version_info.major == 3 and sys.version_info.minor >=9\n",
    "cpu_info = !lscpu\n",
    "is_cpu_ok = \"intel\" in cpu_info.grep(\"vendor\")[0].lower() and \"bf16\" in cpu_info.grep(\"flags\")[0]\n",
    "if not is_os_ok:\n",
    "    print(f\"This notebook requires a Linux type operating system, but you have {os_type}\")\n",
    "if not is_python_ver_ok:\n",
    "    print(f\"This notebook requires Python 3.9+ but your version is {sys.version.split()[0]}.\")\n",
    "if not is_cpu_ok:\n",
    "    print(f\"Your CPU does not support bfloat16.\")\n",
    "print(f\"Notebook requirements {'' if is_os_ok and is_python_ver_ok and is_cpu_ok else 'not'} met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4099797-55d8-492a-94d4-4d815e72aeb8",
   "metadata": {},
   "source": [
    "### Reproducibility Note\n",
    "This notebook was tested on `Ubuntu 22.04.3 LTS` and `Python 3.9.18`.\n",
    "\n",
    "To reproduce the maximum speeds reported in this notebook, please launch it locally, using `numactl`.\\\n",
    "`numactl` enables gaining better control on which resources you're running. It's best to run on a single socket (`-m 0`):\\\n",
    "`OMP_NUM_THREADS=<NUM_THREADS> numactl -C <CORES_RANGE> -m 0 jupyter notebook`\n",
    "\n",
    "For example, if cores 0-59 are available, you can run with one thread per core like so:\\\n",
    "`OMP_NUM_THREADS=60 numactl -C 0-59 -m 0 jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc2a85",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install the required packages to evaluate and visualize results for SetFit models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839bd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U setfit evaluate matplotlib -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cd04-812f-461b-bb26-9225d76c2774",
   "metadata": {},
   "source": [
    "### Set Up Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a090c0-a065-4b3f-b803-5e518c73299f",
   "metadata": {},
   "source": [
    "First, define the infrastructure for conducting latency, throughput and accuracy benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55756fec-fc22-4590-84d7-2f3df37b9256",
   "metadata": {
    "id": "55756fec-fc22-4590-84d7-2f3df37b9256"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import os\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_dir_size(path):\n",
    "    ps = subprocess.Popen(('du','-hk', path), stdout=subprocess.PIPE)\n",
    "    output = subprocess.check_output((\"awk\", \"{print $1/1024,$2}\"), stdin=ps.stdout)\n",
    "    ps.wait()\n",
    "    return float(output.split()[0].decode('utf-8'))\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, model, dataset, optim_type, metric=\"accuracy\", max_sequence_length=512,\n",
    "                 model_path=None, enable_autocast=False, batch_sizes=[1, 4, 8, 16, 32, 64, 128, 256]):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.metric = evaluate.load(metric)\n",
    "        self.model_path = model_path\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.enable_autocast = enable_autocast\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.tokenizer = self.model.model_body.tokenizer\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "    def generate_random_sequences(self, batch_size):\n",
    "        input_ids = torch.randint(0, self.vocab_size - 1, (batch_size, self.max_sequence_length))\n",
    "        token_type_ids = torch.zeros((batch_size, self.max_sequence_length), dtype=torch.int64)\n",
    "        attention_mask = torch.ones((batch_size, self.max_sequence_length), dtype=torch.int64)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    \n",
    "    def compute_accuracy(self):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            self.dataset[\"text\"], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        preds, _ = self.body_head(encoded_inputs)\n",
    "        labels = self.dataset[\"label\"]\n",
    "        accuracy = self.metric.compute(predictions=preds, references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "\n",
    "    def body_head(self, encoded_inputs):\n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "            if isinstance(self.model.model_body, SentenceTransformer):\n",
    "                start = perf_counter()\n",
    "                outputs = self.model.model_body(encoded_inputs)\n",
    "                embeddings = outputs[\"sentence_embedding\"]\n",
    "            else:\n",
    "                start = perf_counter()\n",
    "                outputs = self.model.model_body(**encoded_inputs)\n",
    "                embeddings = mean_pooling(\n",
    "                    outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
    "                )\n",
    "            return self.model.model_head.predict(embeddings), perf_counter() - start\n",
    "\n",
    "    def body(self, encoded_inputs):\n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "            start = perf_counter()\n",
    "            if isinstance(self.model.model_body, SentenceTransformer):\n",
    "                return self.model.model_body(encoded_inputs), perf_counter() - start\n",
    "            else:\n",
    "                return self.model.model_body(**encoded_inputs), perf_counter() - start\n",
    "\n",
    "    def compute_size(self):\n",
    "        if self.model_path is None:\n",
    "            state_dict = self.model.model_body.state_dict()\n",
    "            tmp_path = Path(\"model.pt\")\n",
    "            torch.save(state_dict, tmp_path)\n",
    "            size_mb = get_dir_size(tmp_path)\n",
    "            # Delete temporary file\n",
    "            tmp_path.unlink()\n",
    "        else:\n",
    "            size_mb = get_dir_size(self.model_path)\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def compute_latency_and_throughput(self, num_samples=1000, warmup=50):\n",
    "        res = defaultdict(list)\n",
    "        res[\"batch_sizes\"] = self.batch_sizes\n",
    "\n",
    "        for call_func in self.body, self.body_head:\n",
    "            type = call_func.__name__\n",
    "            print(\"=\" * 100)\n",
    "            for batch_size in self.batch_sizes:\n",
    "                # Warmup\n",
    "                for _ in trange(warmup, file=sys.stdout, desc=f\"Warmup ({type}, {batch_size=})\"):\n",
    "                    inputs = self.generate_random_sequences(batch_size=batch_size)\n",
    "                    _ = call_func(inputs)\n",
    "        \n",
    "                latencies = []\n",
    "                for _ in trange(num_samples // batch_size, file=sys.stdout, desc=f\"Timed Run ({type}, {batch_size=})\"):\n",
    "                    inputs = self.generate_random_sequences(batch_size=batch_size)\n",
    "                    _, latency = call_func(inputs)\n",
    "                    latencies.append(latency)\n",
    "        \n",
    "                # Compute run statistics\n",
    "                latencies = np.array(latencies)\n",
    "                sample_latency = latencies / batch_size\n",
    "                time_avg_ms = 1000 * np.mean(sample_latency)\n",
    "                time_std_ms = 1000 * np.std(sample_latency)\n",
    "                if batch_size == 1:\n",
    "                    res.update({f\"{type}_time_avg_ms\": time_avg_ms, f\"{type}_time_std_ms\": time_std_ms})\n",
    "    \n",
    "                throughputs = batch_size / latencies\n",
    "                throughputs_avg = np.mean(throughputs)\n",
    "                throughputs_std = np.std(throughputs)\n",
    "                res[f\"{type}_throughputs_avg\"].append(throughputs_avg)\n",
    "                res[f\"{type}_throughputs_std\"].append(throughputs_std)\n",
    "                print(f\"{type}: Avg. latency (ms), {batch_size=} - {time_avg_ms:.2f} +/- {time_std_ms:.2f}\")\n",
    "                print(f\"{type}: Avg. throughput (samples/sec), {batch_size=} - {throughputs_avg:.1f} +/- {throughputs_std:.1f}\")\n",
    "        return res\n",
    "        \n",
    "    def run_benchmark(self):\n",
    "        all_metrics = {}\n",
    "        for run_metric in self.compute_latency_and_throughput, self.compute_accuracy, self.compute_size:\n",
    "            all_metrics |= run_metric()\n",
    "        return {self.optim_type: all_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878768f-9ec4-4675-ab20-0f8bca7bbe37",
   "metadata": {},
   "source": [
    "Let's add a function to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99110d37-1aa1-4373-8517-f1f8acbbc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_latency(perf_metrics):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "    plt.figure(figsize=(6, 4), dpi=120)\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        plt.errorbar(\n",
    "            df_opt[\"body_head_time_avg_ms\"],\n",
    "            df_opt[\"accuracy\"] * 100,\n",
    "            xerr=df_opt[\"body_head_time_std_ms\"],\n",
    "            fmt=\"o\",\n",
    "            alpha=0.5,\n",
    "            ms=df_opt[\"size_mb\"] / 15,\n",
    "            label=idx,\n",
    "            capsize=5,\n",
    "            capthick=1,\n",
    "        )\n",
    "    legend = plt.legend(loc=\"lower right\")\n",
    "    plt.ylim(63, 95)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = max([metrics[\"body_head_time_avg_ms\"] for metrics in perf_metrics.values()]) * 1.3\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency with batch_size=1 (ms)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842dea2-76cc-408b-9431-0b282b9abaed",
   "metadata": {},
   "source": [
    "## 1. Benchmark SetFit using PyTorch\n",
    "\n",
    "Load the dataset for running evaluations, and run the benchmark with the standard PyTorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaea6c7-aba5-428c-a625-ddb0bdd2b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=60\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0353a13f1acf473bbd7e798ce2a2a237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (body, batch_size=1):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882590f2dbf14964950003edb5710d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (body, batch_size=1):   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body: Avg. latency (ms), batch_size=1 - 18.90 +/- 1.75\n",
      "body: Avg. throughput (samples/sec), batch_size=1 - 53.3 +/- 3.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee7def1d5164a4eb6829c00d776ea57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (body, batch_size=4):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468edc3208144dfba5dca003f23b1461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (body, batch_size=4):   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body: Avg. latency (ms), batch_size=4 - 9.81 +/- 0.34\n",
      "body: Avg. throughput (samples/sec), batch_size=4 - 102.0 +/- 3.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406d81d9d34b4e5bbba34463de056737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (body, batch_size=8):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73867ddee1284cffa2846ae14c1b329c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (body, batch_size=8):   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body: Avg. latency (ms), batch_size=8 - 8.16 +/- 0.16\n",
      "body: Avg. throughput (samples/sec), batch_size=8 - 122.7 +/- 2.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2ccc54e2bc4d089032870fb4b3be77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (body, batch_size=16):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c3e26150e5496fbabe1b3e2f00ed29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (body, batch_size=16):   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body: Avg. latency (ms), batch_size=16 - 8.41 +/- 0.12\n",
      "body: Avg. throughput (samples/sec), batch_size=16 - 118.9 +/- 1.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284061198448435c809573dec2eec0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (body, batch_size=32):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"].shuffle(seed=42)\n",
    "\n",
    "small_model = SetFitModel.from_pretrained(\"dkorat/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "pb = PerformanceBenchmark(small_model, test_dataset, f\"{device} bge-small (PyTorch)\")\n",
    "!echo OMP_NUM_THREADS=$OMP_NUM_THREADS\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28013b93-e9a9-41ce-b5a9-486f0f4fb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_latency(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AiPUhOCNWRny",
   "metadata": {
    "id": "AiPUhOCNWRny"
   },
   "source": [
    "## 2. CPU: Optimize with Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60d191-db3b-49a3-a281-34ae9063cb25",
   "metadata": {},
   "source": [
    "In order to optimize our SetFit model, we will apply quantization to the model body, using Intel [Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC), part of Optimum Intel.\n",
    "\n",
    "**Quantization** is a very popular deep learning model optimization technique for improving inference speeds. It minimizes the number of bits required to represent the weights and/or activations in a neural network. This is done by converting a set of real-valued numbers into their lower-bit data representations, such as INT8. Moreover, quantization can enable faster computations in lower precision.\n",
    "\n",
    "Specifically, we'll apply post-training static quantization (PTQ). PTQ can reduce the memory footprint and latency for inference, while still preserving the accuracy of the model, with only a small unlabeled calibration set and without any training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167decd-439b-4067-acde-4c3568e99a80",
   "metadata": {},
   "source": [
    "### Install Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95e34d-6f44-4ab6-986d-601ab14f2c60",
   "metadata": {},
   "source": [
    "Install Optimum Intel along with INC and IPEX backends which we'll use in our optimization later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc25364-beee-48d7-b884-4e94b738834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    !python -m pip install --upgrade-strategy eager optimum[neural-compressor] -qqq\n",
    "    !python -m pip install intel-extension-for-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0557f9-8193-4059-8056-bfc50c00deef",
   "metadata": {},
   "source": [
    "#### Prepare a Calibration Dataset\n",
    "\n",
    "The calibration dataset should be able to represent the data distribution of unseen data. In general, preparing 100 samples is enough for calibration. We'll use the Qasper dataset in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8466be-f28d-4303-8511-15798806a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    import random\n",
    "    \n",
    "    def load_qasper_calibration_set(sample_size) -> Dataset:\n",
    "        train_set = load_dataset(\"allenai/qasper\")[\"train\"]\n",
    "        random.seed(666)\n",
    "        random_samples = random.sample(range(len(train_set)), sample_size)\n",
    "        random_queries = [random.sample(train_set[x][\"qas\"][\"question\"], 1)[0] for x in random_samples]\n",
    "        random_abstracts = [train_set[x][\"abstract\"] for x in random_samples]\n",
    "        samples = random.sample(random_queries + random_abstracts, sample_size)\n",
    "        random.shuffle(samples)\n",
    "        def gen():\n",
    "            for s in samples:\n",
    "                yield {\"text\": s}\n",
    "        return Dataset.from_generator(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90c6ba-8f6e-4899-b404-ecebc0f7a78a",
   "metadata": {},
   "source": [
    "#### Run Quantization\n",
    "Define the desired quantization process using `optimum.intel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7739dc-2a56-4960-8f56-7f3f74991fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    import optimum.intel\n",
    "    from neural_compressor.config import PostTrainingQuantConfig\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    \n",
    "    def quantize(model_name, output_path, calibration_set):\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)\n",
    "    \n",
    "        vectorized_ds = calibration_set.map(preprocess_function, num_proc=10)\n",
    "        vectorized_ds = vectorized_ds.remove_columns([\"text\"])\n",
    "    \n",
    "        quantizer = optimum.intel.INCQuantizer.from_pretrained(model)\n",
    "        quantization_config = PostTrainingQuantConfig(approach=\"static\", backend=\"ipex\", domain=\"nlp\")\n",
    "        quantizer.quantize(\n",
    "            quantization_config=quantization_config,\n",
    "            calibration_dataset=vectorized_ds,\n",
    "            save_directory=output_path,\n",
    "            batch_size=1,\n",
    "        )\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "        \n",
    "    model_name = \"dkorat/bge-small-en-v1.5_setfit-sst2-english\"\n",
    "    calibration_set = load_qasper_calibration_set(sample_size=100)\n",
    "    optimum_model_path = f\"/tmp/{model_name}_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f935e3-d09a-4f44-87a0-f773b83e569a",
   "metadata": {},
   "source": [
    "Quantize our SetFit model on 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508125d2-7bc4-4a6a-9b9d-1243ac0ff002",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    quantize(model_name, output_path=optimum_model_path, calibration_set=calibration_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70f73e",
   "metadata": {},
   "source": [
    "Load the optimized model and the test dataset, and perform some inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b2998-6605-4e42-9fb5-247c5eee750f",
   "metadata": {},
   "source": [
    "#### Run Benchmark with Optimized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7215f",
   "metadata": {},
   "source": [
    "Time to run the performance benchmark on our optimized SetFit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8jpZ3gdWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    class ExportedSetFitModel:\n",
    "        def __init__(self, setfit_model, model_body):\n",
    "            model_body.tokenizer = setfit_model.model_body.tokenizer\n",
    "            self.model_body = model_body\n",
    "            self.model_head = setfit_model.model_head\n",
    "    \n",
    "    setfit_model = SetFitModel.from_pretrained(\"dkorat/bge-small-en-v1.5_setfit-sst2-english\")    \n",
    "    optimum_model = optimum.intel.INCModel.from_pretrained(optimum_model_path)\n",
    "    optimum_setfit_model = ExportedSetFitModel(setfit_model, model_body=optimum_model)\n",
    "    \n",
    "    pb = PerformanceBenchmark(\n",
    "        optimum_setfit_model,\n",
    "        test_dataset,\n",
    "        f\"{device}: bge-small (optimum intel)\",\n",
    "        model_path=optimum_model_path,\n",
    "        enable_autocast=True\n",
    "    )\n",
    "    \n",
    "    perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ce857-f4b6-4c14-b3c5-bc6d1b7b33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_latency(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6872f-1765-48a5-beba-a989555b5461",
   "metadata": {},
   "source": [
    "#### Throughput Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e043497-89c0-44a7-b08f-1f8012cd9fba",
   "metadata": {},
   "source": [
    "Let's look now at the throughput, which is the number of samples the model can predict per second.\\\n",
    "We'll plot this value for our optimized and baseline models, as a function of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701770a-d643-4f32-bd56-2574ba7dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    def plot_2_throughputs(perf_metrics, type=\"body_head\"):\n",
    "        xs = np.arange(len(pb.batch_sizes))\n",
    "        plt.figure(figsize=(10, 4), dpi=120)\n",
    "        for (label, metric), color in zip(perf_metrics.items(), [\"darkorange\", \"blue\"]):\n",
    "            y_mean, y_std = np.array(metric[f\"{type}_throughputs_avg\"]), np.array(metric[f\"{type}_throughputs_std\"])\n",
    "            plt.plot(xs, y_mean, label=label, color=color, alpha=0.8)\n",
    "            plt.fill_between(xs, y_mean - y_std, y_mean + y_std, color=color, alpha=0.2)\n",
    "            for x, y in zip(xs, y_mean):\n",
    "                plt.text(x, y, f'{y:.1f}', ha='right', va='bottom', fontsize=7, color=\"black\")\n",
    "        plt.xticks(xs, pb.batch_sizes)\n",
    "        plt.ylabel('Samples/Second')\n",
    "        plt.title(f'Throughput ({type}) vs Batch Size')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "        \n",
    "    plot_2_throughputs(perf_metrics, type=\"body_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d76680-0584-4b89-a26e-6206e459564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_throughputs(perf_metrics, type=\"body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6ab3-0d71-453a-9af3-67f8947fa6ce",
   "metadata": {},
   "source": [
    "#### Similarly to the latency speedup, we can see that our optmization has resulted in up to 3.5x throughput increase as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d76df5-99ce-4ec1-8abe-48a53967dd5c",
   "metadata": {},
   "source": [
    "## 3. GPU: Compressing with Optimum ONNX and CUDAExecutionProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ba054-144d-4778-b312-da0f7b54946f",
   "metadata": {
    "id": "HjeZkCtSqcBe"
   },
   "source": [
    "[`optimum-cli`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimizing-a-model-during-the-onnx-export) makes it extremely easy to export a model to ONNX and apply SOTA graph optimizations / kernel fusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343d77e-a3a8-42a7-b9c9-968fb80389a7",
   "metadata": {
    "cellView": "form",
    "id": "NEnwnsEQWRn8"
   },
   "outputs": [],
   "source": [
    "# !python -m pip install optimum-intel[\"onnxruntime-gpu\"] -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a161fe-bf4d-4e23-992a-6b3e56379069",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPqEcDi8WRn8",
    "outputId": "0e1202d8-aa84-422c-f10f-6bb0b43d1ef8"
   },
   "outputs": [],
   "source": [
    "# !optimum-cli export onnx \\\n",
    "#   --model dkorat/bge-small-en-v1.5_setfit-sst2-english \\\n",
    "#   --task feature-extraction \\\n",
    "#   --optimize O4 \\\n",
    "#   --device cuda \\\n",
    "#   bge_auto_opt_O4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ede73-56e8-4ff0-ba21-c7d7668f7564",
   "metadata": {
    "id": "IYkxQOTRqcBe"
   },
   "source": [
    "We may see some warnings, but these are not ones to be concerned about. We'll see later that it does not affect the model performance.\n",
    "\n",
    "First of all, we'll create a subclass of our performance benchmark to also allow benchmarking ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc7f6c-8a54-4e58-86a3-f344e37fc77d",
   "metadata": {
    "id": "8hvfl3xvlnEs"
   },
   "outputs": [],
   "source": [
    "# class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
    "#     def __init__(self, *args, model_path, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.model_path = model_path\n",
    "\n",
    "#     def compute_size(self):\n",
    "#         size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
    "#         print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "#         return {\"size_mb\": size_mb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cf018-710d-410a-81ca-be429d7a5cc4",
   "metadata": {
    "id": "4ht5U1qUqcBe"
   },
   "source": [
    "Then, we can load the converted SentenceTransformer model with the `\"CUDAExecutionProvider\"` provider. Feel free to also experiment with other providers, such as `\"TensorrtExecutionProvider\"` and `\"CPUExecutionProvider\"`. The former may be even faster than `\"CUDAExecutionProvider\"`, but requires more installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19762fe9-6e59-4de8-8a42-d4c8b6250f63",
   "metadata": {
    "id": "IpoDwkPiWRn8"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "# from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bge_auto_opt_O4', model_max_length=512)\n",
    "# ort_model = ORTModelForFeatureExtraction.from_pretrained('bge_auto_opt_O4', provider=\"CUDAExecutionProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef16445-c3a8-41a5-b787-762789968812",
   "metadata": {
    "id": "Odn2lSPJqcBf"
   },
   "source": [
    "And let's make a class that uses the tokenizer, ONNX Runtime (ORT) model and a model head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed10ea-bfd7-4a42-87e1-28995071e319",
   "metadata": {
    "id": "enaQpBF9WRn9"
   },
   "outputs": [],
   "source": [
    "# from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "\n",
    "# class OnnxSetFitModel:\n",
    "#     def __init__(self, ort_model, tokenizer, model_head):\n",
    "#         self.model_body = ort_model\n",
    "#         self.model_head = model_head\n",
    "\n",
    "#     def predict(self, encoded_inputs):\n",
    "#         outputs = self.model_body(**encoded_inputs)\n",
    "#         embeddings = mean_pooling(\n",
    "#             outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
    "#         )\n",
    "#         return self.model_head.predict(embeddings.cpu())\n",
    "\n",
    "#     def __call__(self, encoded_inputs):\n",
    "#         return self.predict(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588d214-4945-4685-9800-d56a32979311",
   "metadata": {
    "id": "N1TDdcOkqcBh"
   },
   "source": [
    "We can initialize this model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97b3a9-e632-4bf5-8842-bf05b5a5222a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRviEk2WWRn9",
    "outputId": "33f010a8-376e-4f0c-b21b-97fe25bf1a81"
   },
   "outputs": [],
   "source": [
    "# model = SetFitModel.from_pretrained(\"dkorat/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "# onnx_setfit_model = OnnxSetFitModel(ort_model, tokenizer, model.model_head)\n",
    "\n",
    "# # Perform inference\n",
    "# onnx_setfit_model(test_dataset[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a86a9-3ee2-472a-aa71-b021d4a32380",
   "metadata": {
    "id": "3DPl1ZpYqcBh"
   },
   "source": [
    "Time to benchmark this ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24290a40-de87-4232-8eda-fc53e2e19c4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [],
   "source": [
    "# pb = OnnxPerformanceBenchmark(\n",
    "#     onnx_setfit_model,\n",
    "#     test_dataset,\n",
    "#     f\"{device}: bge-small (optimum ONNX)\",\n",
    "#     model_path=\"bge_auto_opt_O4/model.onnx\",\n",
    "# )\n",
    "# perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb93a0-cd0d-41ef-9783-b86052ba2a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "tpjtxQQlZQPa",
    "outputId": "01efad97-4780-4c47-f10f-3afa7e819d15"
   },
   "outputs": [],
   "source": [
    "# plot_metrics(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0403b-1f4b-4952-898e-29e4cc38b569",
   "metadata": {
    "id": "gvdggvIbvowO"
   },
   "source": [
    "By applying ONNX, we were able to improve the latency from 13.43ms per sample to 2.19ms per sample, for a speedup of 6.13x!\n",
    "\n",
    "For further improvements, we recommend increasing the inference batch size, as this may also heavily improve the throughput. For example, setting the batch size to 128 reduces the latency further down to 0.3ms, and down to 0.2ms at a batch size of 2048."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dk-setfit-310",
   "language": "python",
   "name": "dk-setfit-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
